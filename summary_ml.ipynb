{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "Cross-validation is a model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. \n",
    "\n",
    "Model selection: estimating the performance of different models in order to choose the best one.\n",
    "\n",
    "E-Approximation goes down as data size gets bigger and goes up as the model gets more complex\n",
    "\n",
    "Fundamental-tradeoff  :How small can we get the training error vs how well training error approximate test error.\n",
    "\n",
    "Simple model : Decision stump\n",
    "Complex model : Decision tree\n",
    "\n",
    "Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points\n",
    "\n",
    "Underfitting occurs when a statistical model or machine learning algorithm cannot adequately capture the underlying structure of the data\n",
    "\n",
    "Precision = TP/(TP + FP). ‚Ä¢ High precision means the filtered messages are likely to really be spam. ‚Äì Recall: ‚Äúif a message is spam, what is probability it is classified as spam?‚Äù \n",
    "\n",
    " Recall = TP/(TP + FN): ‚Ä¢ High recall means that most spam messages are filtered.\n",
    "\n",
    "\n",
    "By the way, what other methods can predict probabilities in additional to MultiNomial NB?¬∂\n",
    "‚Ä¢\tDecisionTreeClasifier.predict_proba\n",
    "ÔÇß\tComputed as fraction of that class in leaf node.\n",
    "‚Ä¢\tKNeighborsClassifier.predict_proba\n",
    "ÔÇß\tComputed as fraction of that class in   neighbours.\n",
    "By the way, what other methods can predict probabilities?¬∂\n",
    "‚Ä¢\tDecisionTreeClasifier.predict_proba\n",
    "ÔÇß\tComputed as fraction of that class in leaf node.\n",
    "‚Ä¢\tKNeighborsClassifier.predict_proba\n",
    "ÔÇß\tComputed as fraction of that class in   neighbours.\n",
    "‚Ä¢\tDecision trees: only a small number of features are used to make decisions\n",
    "‚Ä¢\t NNs: all features are used equally\n",
    "‚Ä¢\tnaive Bayes: Overly strong conditional independence assumption and overestimates the evidence when there are correlated features.\n",
    "‚Ä¢\tOne of the primary advantage of linear classifiers is their ability to interpret models.\n",
    "\n",
    "Generative vs discriminative model\n",
    "The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classif\n",
    "\n",
    "Choosing a classifier: Logistic regression vs. Naive Bayes¬∂\n",
    "‚Ä¢\tNaive Bayes has overly strong conditional independence assumptions. So not great when features are correlated.\n",
    "ÔÇß\tIf two features are strongly correlated, Naive Bayes will overestimating the evidence of that feature.\n",
    "‚Ä¢\tLogistic regression is much more robust to correlated features\n",
    "ÔÇß\tIf two features are correlated, regression will assign part of the weight to one and part to another.\n",
    "‚Ä¢\tFor smaller datasets Naive Bayes is a good choice. Logistic regression generally works better on larger datasets and is a common default.\n",
    "ÔÇß\tSee this and/or this.\n",
    "‚Ä¢\t\n",
    "Is your data \"linearly separable\"? \n",
    "\n",
    "Why people use linear classifiers?\n",
    "Logistic regression is used EVERYWHERE!\n",
    "‚Ä¢\tFast training and testing.\n",
    "ÔÇß\tTraining on huge datasets.\n",
    "ÔÇß\tTesting is just computing  .\n",
    "‚Ä¢\tWeights   are easy to understand.\n",
    "ÔÇß\tIt's how much   changes the prediction and in what direction.\n",
    "‚Ä¢\tAre somewhat related to neural networks (can be thought of as a 1-layer neural network)\n",
    "‚Ä¢\tWhen I carry out research, logistic regression is the first model I try as a baseline.\n",
    "\n",
    "SVMs: General idea\n",
    "‚Ä¢\tChoose the hyperplane which is furthest away from the closest training points.\n",
    "‚Ä¢\tIn other words, choose the hyperplane which has the largest margin, where margin is the distance from the boundary to the nearest point(s).\n",
    "‚Ä¢\tIntuitively, more margin is good because it leaves more \"room\" before we make an error.\n",
    "‚Ä¢\tgamma controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "‚Ä¢\tlarger gamma   more complex\n",
    "‚Ä¢\tC also affects the fundamental tradeoff, although in a less intuitive way.\n",
    "‚Ä¢\tlarger C   more comp\n",
    "‚Ä¢\tinear_svm = SVC(kernel='linear')\n",
    "‚Ä¢\tlinear_svm.fit(X_train, y_train)\n",
    "‚Ä¢\tshow_scores(linear_svm, X_train, y_train, X_test, y_test)\n",
    "‚Ä¢\t\n",
    "‚Ä¢\t# By dedault SVC uses rbf kernel \n",
    "‚Ä¢\trbf_svm = SVC()\n",
    "‚Ä¢\trbf_svm.fit(X_train, y_train)\n",
    "‚Ä¢\tshow_scores(rbf_svm, X_train, y_train, X_test, y_test)\n",
    "multi-class :\n",
    "‚Ä¢\tNote that SVC multiclass mode is implemented using one-vs-one scheme whereas LinearSVC uses one-vs-rest scheme.\n",
    "Random forests vs decision trees\n",
    "‚Ä¢\tAccuracy\n",
    "ÔÇß\tRandom forests are usually more accurate compared to decision trees, in fact they are usually one of the best performing off-the-shelf classifiers.\n",
    "ÔÇß\tThe original random forests paper by Leo Breiman notes that the error rate depends upon the following:\n",
    "o\tThe correlation between any two trees in the forest. Higher the correlation higher the error rate.\n",
    "o\tThe error rate of each individual tree in the forest. Lowering the error rate of the individual trees decreases the forest error rate.\n",
    "‚Ä¢\tSpeed?\n",
    "ÔÇß\tSlower than decision trees because we are fitting multiple trees\n",
    "ÔÇß\tBut can easily parallelize training because all trees are independent of each other\n",
    "‚Ä¢\tOverfitting\n",
    "ÔÇß\tNo depth decision tree tends to overfit\n",
    "ÔÇß\tRandom forests are less likely to overfit\n",
    "‚Ä¢\tInterpretability\n",
    "ÔÇß\tDecision trees are more interpretable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- If correlation coefficient between two variables is zero then\n",
    "(1) the relationship between the two variables are not linear ,\n",
    "& (2) they are either independent or they are related by some non-linear relationship.\n",
    "\n",
    "\n",
    "A deterministic algorithm is that in which output does not change on different runs. PCA would give the same result if we run again, but not k-mean\n",
    "\n",
    "Bias: difference between training data and model \n",
    "Variance: difference between test data and model \n",
    "\n",
    "Both are true, The OHE will fail to encode the categories which is present in test but not in train so it could be one of the main challenges while applying OHE. The challenge given in option B is also true you need to more careful while applying OHE if frequency distribution doesn't same in train and test\n",
    "-\t\n",
    "\n",
    "\n",
    "Random Forest is a bagging algorithm rather than a boosting algorithm. They are two opposite way to achieve a low error.\n",
    "We know that error can be composited from bias and variance. A too complex model has low bias but large variance, while a too simple model has low variance but large bias, both leading a high error but two different reasons. As a result, two different ways to solve the problem come into people's mind (maybe Breiman and others), variance reduction for a complex model, or bias reduction for a simple model, which refers to random forest and boosting.\n",
    "Random forest reduces variance of a large number of \"complex\" models with low bias. We can see the composition elements are not \"weak\" models but too complex models. If you read about the algorithm, the underlying trees are planted \"somewhat\" as large as \"possible\". The underlying trees are independent parallel models. And additional random variable selection is introduced into them to make them even more independent, which makes it perform better than ordinary bagging and entitle the name \"random\".\n",
    "While boosting reduces bias of a large number of \"small\" models with low variance. They are \"weak\" models as you quoted. The underlying elements are somehow like a \"chain\" or \"nested\" iterative model about the bias of each level. So they are not independent parallel models but each model is built based on all the former small models by weighting. That is so-called \"boosting\" from one by one.\n",
    "Breiman's papers and books discuss about trees, random forest and boosting quite a lot. It helps you to understand the principle behind the algorithm.\n",
    "\n",
    "It's common to find code snippets that treat ùëáT as a hyper-parameter, and attempt to optimize over it in the same way as any other hyper-parameter. This is just wasting computational power: when all other hyper-parameters are fixed, the model‚Äôs loss stochastically decreases as the number of trees increases.\n",
    "Intuitive explanation\n",
    "Each tree in a random forest is identically distributed. The trees are identically distributed because each tree is grown using a randomization strategy that is repeated for each tree: boostrap the training data, and then grow each tree by picking the best split for a feature from among the ùëöm features selected for that node. The random forest procedure stands in contrast to boosting, the trees are grown on their own bootstrap subsample without regard to the other trees. (It is in this sense that the random forest algorithm is \"embarrassingly parallel.\")\n",
    "In the binary case, each random forest tree votes 1 for the positive class or 0 for the negative class for each sample. The average of all of these votes is taken as the classification score of the entire forest. (In the general ùëòk-nary case, we simply have a categorical distribution instead, but all of these arguments still apply.)\n",
    "The Weak Law of Large Numbers is applicable in these circumstances because\n",
    "1.\tthe trees' decisions are identically-distributed r.v.s (in the sense that a random procedure determines whether the tree votes 1 or 0) and\n",
    "2.\tthe variable of interest only takes values {0,1}{0,1} for each tree and therefore each experiment (tree decision) has finite variance (because all moments of countably finite r.v.s are finite).\n",
    "Applying WLLN in this case implies that, for each sample, the ensemble will tend toward a particular mean prediction value for that sample as the number of trees tends towards infinity. Additionally, for a given set of samples, a statistic of interest among those samples (such as the expected log-loss) will converge to a mean value as well, as the number of trees tends toward infinity.\n",
    "\n",
    "The function is a tanh because the this function output range is between (-1,-1).\n",
    "\n",
    "Sigmod btween 0 and 1\n",
    "\n",
    "After adding a feature in feature space, whether that feature is important or unimportant features the R-squared always increase.\n",
    "\n",
    "\n",
    "Weak learners are sure about particular part of a problem. So, they usually don't overfit which means that weak learners have low variance and high bias.\n",
    "We need many of them so we reduce bias.\n",
    "\n",
    "\n",
    "\n",
    "Rf : we can address the problem of descion tree by either reduce bias or variance\n",
    "\n",
    "- reduce variance(over fitting) after growing dt to the max depth\n",
    "- bagging is a method that can be used with any classier that has a bagging problem\n",
    "- noise + bias + variance, your classifer fom the expected classifier, whats the problem with the dt, it is that it keeps splitting and splitting but all these low splits are very data specific, so if you have different data then you would have very different split.\n",
    "\n",
    "- the law of large number , as we increase the the number of DT and averge the result we expect to get the result, the noise will be averged out, as you increase data, then the variannce will decrease, remember if yoy have infinited data then there is no overfitting.\n",
    "\n",
    "- Bootstrapping \" sample with repalcement\", is then used to generate data set out of the original data set\n",
    "\n",
    "- Benefits : has a higher accuracy\n",
    "             also it does not require lots of preprocessing\n",
    "             \n",
    "- Baggin is good wheneveer you have a problem of high variance\n",
    "             \n",
    "   \n",
    "### Boosting:\n",
    "\n",
    "- solve the porblem if you have weak learners. use train them sequentially\n",
    "\n",
    "- very similar to the gradient descenet taking many small steps, gradietn desent in function sapce. Every step I am actually making samll progress toward the right direction\n",
    "- iteratively train new learners to overcome the shortcoming of the previous leaners.\n",
    "- weak learners can not overfit, and also can not learn complex problems\n",
    "\n",
    "### PCA\n",
    "\n",
    "-  When the data has a zero mean, then the the PCA will have the same projection as the SVD, othewise we have to center the data to get the same result.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that you will definitely need : \n",
    "\n",
    "`(X_train, X_vt, y_train, y_vt) = train_test_split(X, y, test_size=0.4, random_state=0)`\n",
    "\n",
    "`(X_validation, X_test, y_validation, y_test) = train_test_split(X_vt, y_vt, test_size=0.5, random_state=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
